# Date: 28.02
## Current Todos
-> 90min each
1. [DONE] Count of reached target seems not to work as intended  
2. Ask in the RL group + tutor about the random seeds 
and think how to implement them in your project [DONE]
3. Fix class for Trainable and test random seed with tune and ray
4. Add policy entropy to your project


### Todo 1  [DONE]
It seems like the the episode ends before the target is reached, 
the picture shows the agent almost at the target. [DONE]

Hypothesis - Order of next try:

- Maybe it is also the < sign in the "at_target" method comparison --> replace with <= [DONE]
- Maybe the problem is the episode_done callback? use another callback? 
- Maybe the check is not done because the episode already ends?

Solution: 
The  constant GuidanceEnv.MIN_DISTANCE_TO_TARGET_KM was used in places 
where this.target_radius should have been used in the environment 


### Todo 2
I've asked pascal and read different articels on it. It got clearer now. 
A seed is used to make experiments reproducible by using always the same "random" numbers.
Need to test it with ray and my custom env however... before pushing

Frage:
Hallo Herr Titze! Dürfte ich Sie fragen, wie sie selbst vorgehen würden, 
um zwei Algorithmen (oder sogar die selben Algorithmen, aber mit unterschiedlichen Hyperparametern) 
zu vergleichen / evaluieren? 
Schauen Sie sich grob nur die mean rewards von mehreren Läufen an und nehmen davon nochmal 
den Durchschnitt um diese zu vergleichen? 
Oder vielleicht Welch's t-test (https://en.wikipedia.org/wiki/Welch%27s_t-test)?

```
# Code
# More code
```

Some image of tensorboard:

![Test Image 1](images/3DTest.png)